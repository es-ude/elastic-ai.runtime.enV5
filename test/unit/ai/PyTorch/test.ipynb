{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-07T07:36:09.566349Z",
     "start_time": "2025-10-07T07:36:06.825244Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Parameter, Softmax, ReLU, Linear, Sequential, MSELoss"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T07:36:10.577227Z",
     "start_time": "2025-10-07T07:36:10.559747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = nn.Softmax(dim=0)\n",
    "x = nn.Parameter(Tensor([-1., 0., 1., 2., 5., -6.]), requires_grad=True)\n",
    "y = f(x)\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    target = Tensor([2.e-03, 7.e-03, 2e-02, 7.e-02, 9.e-01, 1.e-3])\n",
    "    \n",
    "print(target.sum())\n",
    "    \n",
    "def loss_fn(output, target):\n",
    "    return torch.sum(-torch.log(output) * target)\n",
    "\n",
    "loss = loss_fn(y, target)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "x.grad.zero_()\n",
    "\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "loss2 = loss_fn2(x, target)\n",
    "loss2.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "\n"
   ],
   "id": "d6bc0736b964dc8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n",
      "tensor([2.3008e-03, 6.2543e-03, 1.7001e-02, 4.6213e-02, 9.2822e-01, 1.5503e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1.0000)\n",
      "tensor(0.4225, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0003, -0.0007, -0.0030, -0.0238,  0.0282, -0.0010])\n",
      "tensor([ 0.0003, -0.0007, -0.0030, -0.0238,  0.0282, -0.0010])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T15:47:22.214937Z",
     "start_time": "2025-10-06T15:47:22.209898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = nn.Parameter(Tensor([0.8, 0.2, 0.]), requires_grad=True)\n",
    "target = Tensor([0.9, 0.1, 0.])\n",
    "\n",
    "loss = loss_fn(x, target)\n",
    "\n",
    "loss.backward()\n",
    "print(x.grad)"
   ],
   "id": "72aeadc6c4f72d79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1250, -0.5000,     nan])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T15:59:21.240812Z",
     "start_time": "2025-10-06T15:59:21.233060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = nn.Parameter(Tensor([2., 1., 0.1]), requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    target = Tensor([0.9, 0.1, 0.])\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(x, target)\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n"
   ],
   "id": "a2a4a7628f0e5a40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2410,  0.1424,  0.0986])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "505ecf755c641824"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d13d16b959267640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T15:18:04.755178Z",
     "start_time": "2025-09-23T15:18:04.747074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "\n",
    "target1 = torch.tensor([0])\n",
    "fake_param1 = Parameter(torch.zeros_like(logits), requires_grad=True)\n",
    "loss1 = criterion(logits+fake_param1, target1)\n",
    "print(loss1.item())\n",
    "loss1.backward()\n",
    "print(fake_param1.grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "target2 = torch.tensor([[0.9, 0.1, 0.]])\n",
    "fake_param2 = Parameter(torch.zeros_like(logits), requires_grad=True)\n",
    "loss2 = criterion(logits+fake_param2, target2)\n",
    "print(loss2.item())\n",
    "loss2.backward()\n",
    "print(fake_param2.grad)\n",
    "\n"
   ],
   "id": "31fd349ba58974c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652\n",
      "tensor([[-0.3410,  0.2424,  0.0986]])\n",
      "\n",
      "0.5170299410820007\n",
      "tensor([[-0.2410,  0.1424,  0.0986]])\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T07:54:26.462507Z",
     "start_time": "2025-10-02T07:54:26.357340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv1d = nn.Conv1d(in_channels=2, out_channels=3, kernel_size=3, padding=0)\n",
    "\n",
    "model = Sequential(conv1d)\n",
    "model[0].weight = Parameter(\n",
    "    Tensor([[[-1., 0., 1.], [-1., 2., 3.]], \n",
    "            [[0., 0., 0.], [1., -1., -2.]], \n",
    "            [[-2., -1., -2.], [-3., -1., 0.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.,1.,2.]))\n",
    "\n",
    "x = Tensor([[ [0.,1.,2.,3.,4.,5.], [0.,-1.,-2.,-3.,-4.,-5.] ]])\n",
    "x.requires_grad = True\n",
    "y = model(x)\n",
    "print(y)\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = y+Tensor([[ [-1.,0.,-2.,-1.], [1.,0.,-2.,-4.], [-2.,-1.,0.,-1.] ]])\n",
    "print(target)\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(\"FIRST CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n",
    "print(\"\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(\"SECOND CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n"
   ],
   "id": "8cfd923623c6c321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -2.,  -6., -10., -14., -18., -10.],\n",
      "         [  3.,   6.,   8.,  10.,  12.,   2.],\n",
      "         [  0.,  -2.,  -3.,  -4.,  -5.,   6.]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m loss_fn = MSELoss()\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     target = \u001B[43my\u001B[49m\u001B[43m+\u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m4.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(target)\n\u001B[32m     18\u001B[39m loss = loss_fn(y, target)\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T12:26:25.267560Z",
     "start_time": "2025-09-26T12:26:25.259912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear = nn.Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "linear.weight = Parameter(torch.tensor([[-1.,  2., -3.],\n",
    "                                        [ 4.,  5., -6.]]))\n",
    "\n",
    "linear.bias = Parameter(torch.tensor([-1., 3.]))  \n",
    "\n",
    "model = nn.Sequential(linear)\n",
    "\n",
    "x = Parameter(Tensor([0., 1., 2.]))\n",
    "y = model(x)\n",
    "print(y)\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = Tensor([-4., -3.])\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "3030b8378d87360f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5., -4.], grad_fn=<ViewBackward0>)\n",
      "tensor([-3., -7.,  9.])\n",
      "tensor([ -6., -14.,  18.])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T12:26:17.064155Z",
     "start_time": "2025-09-26T12:26:17.055682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([0., 1., 2.], requires_grad=True)\n",
    "grad_out = torch.tensor([-4., -3.])\n",
    "\n",
    "linear = Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(torch.tensor([[-1.,  2., -3.],\n",
    "                                      [ 4.,  5., -6.]]))\n",
    "    linear.bias.copy_(torch.tensor([-1., 3.]))\n",
    "\n",
    "y = linear(x)\n",
    "\n",
    "y.backward(grad_out)\n",
    "\n",
    "print(\"propagated_loss:\")\n",
    "print(x.grad)\n",
    "\n",
    "print(\"weight.grad:\")\n",
    "print(linear.weight.grad)\n",
    "\n",
    "print(\"bias.grad:\")\n",
    "print(linear.bias.grad)\n"
   ],
   "id": "6f61f099dcbd7534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propagated_loss:\n",
      "tensor([ -8., -23.,  30.])\n",
      "weight.grad:\n",
      "tensor([[-0., -4., -8.],\n",
      "        [-0., -3., -6.]])\n",
      "bias.grad:\n",
      "tensor([-4., -3.])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:20:15.461380Z",
     "start_time": "2025-09-29T14:20:15.404460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Input\n",
    "x = Tensor([[[0., 1., 2., 3., 4., 5.],\n",
    "             [0., -1., -2., -3., -4., -5.]]])  # shape: (batch=1, in_channels=2, length=6)\n",
    "x.requires_grad = True\n",
    "\n",
    "# Conv1d layer\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    dilation=2,\n",
    "    padding=1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# Set weights manually\n",
    "conv1d.weight = Parameter(torch.tensor([\n",
    "    [[-1., 0., 1.], [-1., 2., 3.]],    # out_channel 0\n",
    "    [[0., 0., 0.], [1., -1., -2.]],    # out_channel 1\n",
    "    [[-2., -1., -2.], [-3., -1., 0.]]  # out_channel 2\n",
    "], dtype=torch.float32))\n",
    "\n",
    "conv1d.bias = Parameter(torch.tensor([0., 1., 2.], dtype=torch.float32))\n",
    "\n",
    "# Sequential model\n",
    "model = nn.Sequential(conv1d)\n",
    "\n",
    "# Forward pass\n",
    "y = model(x)\n",
    "print(\"Output y:\")\n",
    "print(y)\n",
    "\n",
    "# Define target for MSE loss\n",
    "target = y + Tensor([[[ -1., 0., -2., -1.],\n",
    "                      [ 1., 0., -2., -4.],\n",
    "                      [ -2., -1., 0., -1.]]])\n",
    "\n",
    "# Compute loss and backward\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Input gradient x.grad:\")\n",
    "print(x.grad)\n"
   ],
   "id": "35f6b4f5509a675a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y:\n",
      "tensor([[[-8., -8.],\n",
      "         [ 8.,  3.],\n",
      "         [-4.,  4.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 35\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;28mprint\u001B[39m(y)\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# Define target for MSE loss\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m target = \u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m                      \u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m4.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m                      \u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Compute loss and backward\u001B[39;00m\n\u001B[32m     40\u001B[39m loss_fn = nn.MSELoss()\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T08:50:59.472970Z",
     "start_time": "2025-10-02T08:50:59.449248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "x = Tensor([[[0., 1., 2., 3., 4., 5.],      # channel 0\n",
    "             [0., -1., -2., -3., -4., -5.]]])  # channel 1\n",
    "\n",
    "x.requires_grad = True\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    dilation=2,\n",
    "    padding=1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = Sequential(conv1d)\n",
    "model[0].weight = Parameter(\n",
    "    Tensor([[[-1., 0., 1.], [-1., 2., 3.]], \n",
    "            [[0., 0., 0.], [1., -1., -2.]], \n",
    "            [[-2., -1., -2.], [-3., -1., 0.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.,1.,2.]))\n",
    "\n",
    "#print(\"\\nWeight shape:\", conv1d.weight.shape)\n",
    "#print(\"\\nBias:\", conv1d.bias.tolist())\n",
    "\n",
    "x = Tensor([[ [0.,1.,2.,3.,4.,5.], [0.,-1.,-2.,-3.,-4.,-5.] ]])\n",
    "x.requires_grad = True\n",
    "y = model(x)\n",
    "print(y)\n",
    "print(\"Output shape:\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = Tensor([[ [-4.,-3.],[-2.,-1.], [0.,1.] ]])\n",
    "print(target)\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(\"FIRST CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n",
    "print(\"\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(\"SECOND CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n"
   ],
   "id": "2ef9fed02c5ca971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 2, 6])\n",
      "tensor([[[-8., -8.],\n",
      "         [ 8.,  3.],\n",
      "         [-4.,  4.]]], grad_fn=<ConvolutionBackward0>)\n",
      "Output shape: torch.Size([1, 3, 2])\n",
      "tensor([[[-4., -3.],\n",
      "         [-2., -1.],\n",
      "         [ 0.,  1.]]])\n",
      "FIRST CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[  0.0000,   1.3333,  -0.3333,   1.3333,  -1.0000,   0.0000],\n",
      "         [  0.0000,  -4.6667,   0.0000, -10.6667,  -5.6667,   0.0000]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ -3.3333,  -8.0000,  -4.0000],\n",
      "         [  3.3333,   8.0000,   4.0000]],\n",
      "\n",
      "        [[  2.6667,   8.6667,  10.0000],\n",
      "         [ -2.6667,  -8.6667, -10.0000]],\n",
      "\n",
      "        [[  2.0000,   2.6667,  -4.0000],\n",
      "         [ -2.0000,  -2.6667,   4.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([-3.0000,  4.6667, -0.3333])\n",
      "\n",
      "--------------------------\n",
      "SECOND CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[  0.0000,   1.3333,  -0.3333,   1.3333,  -1.0000,   0.0000],\n",
      "         [  0.0000,  -4.6667,   0.0000, -10.6667,  -5.6667,   0.0000]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ -3.3333,  -8.0000,  -4.0000],\n",
      "         [  3.3333,   8.0000,   4.0000]],\n",
      "\n",
      "        [[  2.6667,   8.6667,  10.0000],\n",
      "         [ -2.6667,  -8.6667, -10.0000]],\n",
      "\n",
      "        [[  2.0000,   2.6667,  -4.0000],\n",
      "         [ -2.0000,  -2.6667,   4.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([-3.0000,  4.6667, -0.3333])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:10:06.973246Z",
     "start_time": "2025-09-29T14:10:06.967593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = torch.tensor([1., 2., -3.], requires_grad=True)\n",
    "label = torch.tensor([-5., -4., 2.])\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "loss = loss_fn(output, label)\n",
    "loss.backward()\n",
    "\n",
    "print(output.grad)\n"
   ],
   "id": "3519318d9a898a03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.0000,  4.0000, -3.3333])\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:39:55.638364Z",
     "start_time": "2025-10-02T13:39:55.627060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[[0., 1., 2., 3., 4., 5.],\n",
    "                   [0., -1., -2., -3., -4., -5.]]], requires_grad=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv1d(in_channels=2, out_channels=3, kernel_size=3, dilation=2, stride=1, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=6, out_features=2)\n",
    ")\n",
    "\n",
    "model[0].weight = Parameter(\n",
    "    Tensor([[[-1., 0., 1.], [-1., 2., 3.]], \n",
    "            [[0., 0., 0.], [1., -1., -2.]], \n",
    "            [[-2., -1., -2.], [-3., -1., 0.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.,1.,2.]))\n",
    "\n",
    "model[3].weight = Parameter(torch.tensor([\n",
    "    [1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1.],\n",
    "    [-1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0.]\n",
    "], dtype=torch.float32))\n",
    "model[3].bias = Parameter(torch.tensor([0., 1.], dtype=torch.float32))\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    target = torch.tensor([[0.1, 0.9]])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(y, target)\n",
    "\n",
    "print(\"Model output:\", y)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "loss.backward()\n",
    "print(\"Input gradient x.grad:\")\n",
    "print(x.grad)\n"
   ],
   "id": "bcb54b01cbe0a16d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: tensor([[-1., -2.]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 1.213261604309082\n",
      "Input gradient x.grad:\n",
      "tensor([[[ 0.0000,  0.0000,  1.2621,  1.2621,  0.6311,  0.6311],\n",
      "         [-1.8932,  1.8932, -0.6311,  1.2621,  2.5242, -1.2621]]])\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Input: 1 Batch, 1 Channel, Länge 6\n",
    "x = torch.tensor([[[0., 1., 2., 3., 4., 5.]]], requires_grad=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, dilation=2, stride=1, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=6, out_features=1)  # Output-Größe = Input-Größe\n",
    ")\n",
    "\n",
    "# Conv1d-Gewicht: (out_channels=1, in_channels=1, kernel_size=3)\n",
    "model[0].weight = Parameter(Tensor([[[-1., 0., 1.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.]))\n",
    "\n",
    "# Linear-Gewicht: (out_features=6, in_features=6)\n",
    "model[3].weight = Parameter(torch.tensor([\n",
    "    [-1., 0., 1., -1., 2., 3.]\n",
    "], dtype=torch.float32))\n",
    "model[3].bias = Parameter(torch.tensor([-1.], dtype=torch.float32))\n",
    "\n",
    "# Forward\n",
    "y = model(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Conv output shape:\", model[0](x).shape)  # vor Flatten\n",
    "print(\"Model output shape:\", y.shape)\n",
    "print(\"Model output:\", y)\n",
    "\n",
    "# Loss\n",
    "target = torch.tensor([[-4.]])  # gleiche Größe wie Output\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y, target)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "print(\"Input gradient x.grad:\")\n",
    "print(x.grad)\n"
   ],
   "id": "2ea0c428668fdaea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T07:59:53.476466Z",
     "start_time": "2025-10-02T07:59:53.437094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv1d = nn.Conv1d(in_channels=2, out_channels=3, kernel_size=3, padding=\"same\")\n",
    "\n",
    "model = Sequential(conv1d)\n",
    "model[0].weight = Parameter(\n",
    "    Tensor([[[-1., 0., 1.], [-1., 2., 3.]], \n",
    "            [[0., 0., 0.], [1., -1., -2.]], \n",
    "            [[-2., -1., -2.], [-3., -1., 0.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.,1.,2.]))\n",
    "\n",
    "x = Tensor([[ [0.,1.,2.,3.,4.,5.], [0.,-1.,-2.,-3.,-4.,-5.] ]])\n",
    "x.requires_grad = True\n",
    "y = model(x)\n",
    "print(y)\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = y+Tensor([[[ -2.,  -6., -10., -14., -18., -10.],\n",
    "         [  3.,   6.,   8.,  10.,  12.,   2.],\n",
    "         [  0.,  -2.,  -3.,  -4.,  -5.,   6.]]])\n",
    "print(target)\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(\"FIRST CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n",
    "print(\"\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(\"SECOND CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n"
   ],
   "id": "e94889651042a3e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -2.,  -6., -10., -14., -18., -10.],\n",
      "         [  3.,   6.,   8.,  10.,  12.,   2.],\n",
      "         [  0.,  -2.,  -3.,  -4.,  -5.,   6.]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[ -4., -12., -20., -28., -36., -20.],\n",
      "         [  6.,  12.,  16.,  20.,  24.,   4.],\n",
      "         [  0.,  -4.,  -6.,  -8., -10.,  12.]]])\n",
      "FIRST CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[-1.1111, -1.7778, -2.5556, -3.1111,  0.3333,  1.5556],\n",
      "         [-1.2222,  0.1111,  2.1111,  3.8889, 12.3333, 11.7778]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ 14.6667,  21.1111,  21.1111],\n",
      "         [-14.6667, -21.1111, -21.1111]],\n",
      "\n",
      "        [[ -8.0000, -12.2222, -15.4444],\n",
      "         [  8.0000,  12.2222,  15.4444]],\n",
      "\n",
      "        [[  0.2222,   1.1111,   6.0000],\n",
      "         [ -0.2222,  -1.1111,  -6.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([ 6.6667, -4.5556,  0.8889])\n",
      "\n",
      "--------------------------\n",
      "SECOND CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[-1.1111, -1.7778, -2.5556, -3.1111,  0.3333,  1.5556],\n",
      "         [-1.2222,  0.1111,  2.1111,  3.8889, 12.3333, 11.7778]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ 14.6667,  21.1111,  21.1111],\n",
      "         [-14.6667, -21.1111, -21.1111]],\n",
      "\n",
      "        [[ -8.0000, -12.2222, -15.4444],\n",
      "         [  8.0000,  12.2222,  15.4444]],\n",
      "\n",
      "        [[  0.2222,   1.1111,   6.0000],\n",
      "         [ -0.2222,  -1.1111,  -6.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([ 6.6667, -4.5556,  0.8889])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T13:32:21.569475Z",
     "start_time": "2025-10-02T13:32:21.554247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# ----------------------------\n",
    "# Input: 1 Batch, 2 Channels, Länge 6\n",
    "# ----------------------------\n",
    "x = torch.tensor([[[0., 1., 2., 3., 4., 5.],\n",
    "                   [0., -1., -2., -3., -4., -5]]], requires_grad=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Modell: Conv1d -> ReLU -> Linear\n",
    "# ----------------------------\n",
    "model = nn.Sequential(\n",
    "    nn.Conv1d(in_channels=2, out_channels=3, kernel_size=3, stride=1, dilation=2, padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=3*6, out_features=2)\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Gewichte manuell setzen (deterministisch)\n",
    "# ----------------------------\n",
    "# Conv1d: (out_channels=3, in_channels=2, kernel_size=3)\n",
    "conv_weights = torch.tensor([\n",
    "    [[1., 0., -1.], [0., 1., 0.]],      # Out-Channel 0\n",
    "    [[-1., 1., 0.], [1., -1., 1.]],     # Out-Channel 1\n",
    "    [[0., -1., 1.], [1., 0., -1.]]      # Out-Channel 2\n",
    "], dtype=torch.float32)\n",
    "conv_bias = torch.tensor([0., 1., -1.], dtype=torch.float32)\n",
    "\n",
    "model[0].weight = Parameter(conv_weights)\n",
    "model[0].bias = Parameter(conv_bias)\n",
    "\n",
    "# Linear: (out_features=2, in_features=18)\n",
    "linear_weights = torch.tensor([\n",
    "    [1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1.],\n",
    "    [-1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0., -1., 1., 0.]\n",
    "], dtype=torch.float32)\n",
    "linear_bias = torch.tensor([0., 1.], dtype=torch.float32)\n",
    "\n",
    "model[3].weight = Parameter(linear_weights)\n",
    "model[3].bias = Parameter(linear_bias)\n",
    "\n",
    "# ----------------------------\n",
    "# Forward-Pass\n",
    "# ----------------------------\n",
    "y = model(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Conv output shape (vor Flatten):\", model[0](x).shape)\n",
    "print(\"Model output shape:\", y.shape)\n",
    "print(\"Model output:\", y)\n",
    "\n",
    "# ----------------------------\n",
    "# Target\n",
    "# ----------------------------\n",
    "target = torch.tensor([[-4., -3.]], dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Loss\n",
    "# ----------------------------\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y, target)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# ----------------------------\n",
    "# Backward-Pass\n",
    "# ----------------------------\n",
    "loss.backward()\n",
    "print(\"Gradient w.r.t input x.grad:\")\n",
    "print(x.grad)\n",
    "\n",
    "# Optional: Gradienten der Gewichte\n",
    "print(\"Gradient Conv1d weight.grad:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient Linear weight.grad:\")\n",
    "print(model[3].weight.grad)\n"
   ],
   "id": "63a0e012495ce884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 2, 6])\n",
      "Conv output shape (vor Flatten): torch.Size([1, 3, 6])\n",
      "Model output shape: torch.Size([1, 2])\n",
      "Model output: tensor([[-3.,  2.]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 13.0\n",
      "Gradient w.r.t input x.grad:\n",
      "tensor([[[ 5., -5., -9., 10.,  4., -5.],\n",
      "         [-2., -4., 10., -6., -5.,  5.]]])\n",
      "Gradient Conv1d weight.grad:\n",
      "tensor([[[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  7.,  13.,  -4.],\n",
      "         [ -7., -13.,   4.]],\n",
      "\n",
      "        [[ -4.,  -9., -17.],\n",
      "         [  4.,   9.,  17.]]])\n",
      "Gradient Linear weight.grad:\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  5.,  5.,  3.,  4.,\n",
      "          5.,  5.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0., 25., 25., 15., 20.,\n",
      "         25., 25.,  0.,  0.]])\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
