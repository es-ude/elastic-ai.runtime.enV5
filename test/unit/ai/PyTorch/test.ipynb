{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-29T10:05:55.159113Z",
     "start_time": "2025-09-29T10:05:55.155685Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Parameter, Softmax, ReLU, Linear, Sequential, MSELoss"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T12:03:42.693022Z",
     "start_time": "2025-09-23T12:03:42.686260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = nn.Softmax(dim=0)\n",
    "x = nn.Parameter(Tensor([-1., 0., 1., 2., 5., -6.]), requires_grad=True)\n",
    "y = f(x)\n",
    "print(y)\n",
    "\n",
    "test = nn.Parameter(Tensor([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]]))\n",
    "print(test.shape)\n",
    "with torch.no_grad():\n",
    "    target = y+ Tensor([-1., 0., 1., 1, -5, -1.])\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad)"
   ],
   "id": "d6bc0736b964dc8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3008e-03, 6.2543e-03, 1.7001e-02, 4.6213e-02, 9.2822e-01, 1.5503e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([-2.7458e-03, -9.5486e-03, -3.1623e-02, -8.5959e-02,  1.2989e-01,\n",
      "        -1.8501e-05])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d13d16b959267640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T15:18:04.755178Z",
     "start_time": "2025-09-23T15:18:04.747074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "\n",
    "target1 = torch.tensor([0])\n",
    "fake_param1 = Parameter(torch.zeros_like(logits), requires_grad=True)\n",
    "loss1 = criterion(logits+fake_param1, target1)\n",
    "print(loss1.item())\n",
    "loss1.backward()\n",
    "print(fake_param1.grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "target2 = torch.tensor([[0.9, 0.1, 0.]])\n",
    "fake_param2 = Parameter(torch.zeros_like(logits), requires_grad=True)\n",
    "loss2 = criterion(logits+fake_param2, target2)\n",
    "print(loss2.item())\n",
    "loss2.backward()\n",
    "print(fake_param2.grad)\n",
    "\n"
   ],
   "id": "31fd349ba58974c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652\n",
      "tensor([[-0.3410,  0.2424,  0.0986]])\n",
      "\n",
      "0.5170299410820007\n",
      "tensor([[-0.2410,  0.1424,  0.0986]])\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T10:25:18.734963Z",
     "start_time": "2025-09-29T10:25:18.722383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv1d = nn.Conv1d(in_channels=2, out_channels=3, kernel_size=3, padding=0)\n",
    "\n",
    "model = Sequential(conv1d)\n",
    "model[0].weight = Parameter(\n",
    "    Tensor([[[-1., 0., 1.], [-1., 2., 3.]], \n",
    "            [[0., 0., 0.], [1., -1., -2.]], \n",
    "            [[-2., -1., -2.], [-3., -1., 0.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.,1.,2.]))\n",
    "\n",
    "x = Tensor([[ [0.,1.,2.,3.,4.,5.], [0.,-1.,-2.,-3.,-4.,-5.] ]])\n",
    "x.requires_grad = True\n",
    "y = model(x)\n",
    "print(y)\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = y+Tensor([[ [-1.,0.,-2.,-1.], [1.,0.,-2.,-4.], [-2.,-1.,0.,-1.] ]])\n",
    "print(target)\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(\"FIRST CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n",
    "print(\"\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(\"SECOND CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n"
   ],
   "id": "8cfd923623c6c321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6., -10., -14., -18.],\n",
      "         [  6.,   8.,  10.,  12.],\n",
      "         [ -2.,  -3.,  -4.,  -5.]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[ -7., -10., -16., -19.],\n",
      "         [  7.,   8.,   8.,   8.],\n",
      "         [ -4.,  -4.,  -4.,  -6.]]])\n",
      "FIRST CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[-0.8333, -0.6667, -1.0000, -0.8333,  0.1667, -0.1667],\n",
      "         [-1.3333, -0.3333,  0.6667,  0.3333, -0.1667, -0.8333]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ 1.1667,  1.8333,  2.5000],\n",
      "         [-1.1667, -1.8333, -2.5000]],\n",
      "\n",
      "        [[ 2.6667,  3.5000,  4.3333],\n",
      "         [-2.6667, -3.5000, -4.3333]],\n",
      "\n",
      "        [[ 0.6667,  1.3333,  2.0000],\n",
      "         [-0.6667, -1.3333, -2.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([0.6667, 0.8333, 0.6667])\n",
      "\n",
      "--------------------------\n",
      "SECOND CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[-0.8333, -0.6667, -1.0000, -0.8333,  0.1667, -0.1667],\n",
      "         [-1.3333, -0.3333,  0.6667,  0.3333, -0.1667, -0.8333]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ 1.1667,  1.8333,  2.5000],\n",
      "         [-1.1667, -1.8333, -2.5000]],\n",
      "\n",
      "        [[ 2.6667,  3.5000,  4.3333],\n",
      "         [-2.6667, -3.5000, -4.3333]],\n",
      "\n",
      "        [[ 0.6667,  1.3333,  2.0000],\n",
      "         [-0.6667, -1.3333, -2.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([0.6667, 0.8333, 0.6667])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T12:26:25.267560Z",
     "start_time": "2025-09-26T12:26:25.259912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear = nn.Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "linear.weight = Parameter(torch.tensor([[-1.,  2., -3.],\n",
    "                                        [ 4.,  5., -6.]]))\n",
    "\n",
    "linear.bias = Parameter(torch.tensor([-1., 3.]))  \n",
    "\n",
    "model = nn.Sequential(linear)\n",
    "\n",
    "x = Parameter(Tensor([0., 1., 2.]))\n",
    "y = model(x)\n",
    "print(y)\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = Tensor([-4., -3.])\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "3030b8378d87360f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5., -4.], grad_fn=<ViewBackward0>)\n",
      "tensor([-3., -7.,  9.])\n",
      "tensor([ -6., -14.,  18.])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T12:26:17.064155Z",
     "start_time": "2025-09-26T12:26:17.055682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([0., 1., 2.], requires_grad=True)\n",
    "grad_out = torch.tensor([-4., -3.])\n",
    "\n",
    "linear = Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(torch.tensor([[-1.,  2., -3.],\n",
    "                                      [ 4.,  5., -6.]]))\n",
    "    linear.bias.copy_(torch.tensor([-1., 3.]))\n",
    "\n",
    "y = linear(x)\n",
    "\n",
    "y.backward(grad_out)\n",
    "\n",
    "print(\"propagated_loss:\")\n",
    "print(x.grad)\n",
    "\n",
    "print(\"weight.grad:\")\n",
    "print(linear.weight.grad)\n",
    "\n",
    "print(\"bias.grad:\")\n",
    "print(linear.bias.grad)\n"
   ],
   "id": "6f61f099dcbd7534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propagated_loss:\n",
      "tensor([ -8., -23.,  30.])\n",
      "weight.grad:\n",
      "tensor([[-0., -4., -8.],\n",
      "        [-0., -3., -6.]])\n",
      "bias.grad:\n",
      "tensor([-4., -3.])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T14:09:50.762606Z",
     "start_time": "2025-09-26T14:09:50.502851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Input\n",
    "x = Tensor([[[0., 1., 2., 3., 4., 5.],\n",
    "             [0., -1., -2., -3., -4., -5.]]])  # shape: (batch=1, in_channels=2, length=6)\n",
    "x.requires_grad = True\n",
    "\n",
    "# Conv1d layer\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    dilation=2,\n",
    "    padding=1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# Set weights manually\n",
    "conv1d.weight = Parameter(torch.tensor([\n",
    "    [[-1., 0., 1.], [-1., 2., 3.]],    # out_channel 0\n",
    "    [[0., 0., 0.], [1., -1., -2.]],    # out_channel 1\n",
    "    [[-2., -1., -2.], [-3., -1., 0.]]  # out_channel 2\n",
    "], dtype=torch.float32))\n",
    "\n",
    "conv1d.bias = Parameter(torch.tensor([0., 1., 2.], dtype=torch.float32))\n",
    "\n",
    "# Sequential model\n",
    "model = nn.Sequential(conv1d)\n",
    "\n",
    "# Forward pass\n",
    "y = model(x)\n",
    "print(\"Output y:\")\n",
    "print(y)\n",
    "\n",
    "# Define target for MSE loss\n",
    "target = y + Tensor([[[ -1., 0., -2., -1.],\n",
    "                      [ 1., 0., -2., -4.],\n",
    "                      [ -2., -1., 0., -1.]]])\n",
    "\n",
    "# Compute loss and backward\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Input gradient x.grad:\")\n",
    "print(x.grad)\n"
   ],
   "id": "35f6b4f5509a675a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y:\n",
      "tensor([[[-8., -8.],\n",
      "         [ 8.,  3.],\n",
      "         [-4.,  4.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/Arbeit/elastic-ai.runtime.enV5/python_utils/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([1, 3, 4])) that is different to the input size (torch.Size([1, 3, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 41\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Compute loss and backward\u001B[39;00m\n\u001B[32m     40\u001B[39m loss_fn = nn.MSELoss()\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m loss = \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     42\u001B[39m loss.backward()\n\u001B[32m     44\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mInput gradient x.grad:\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Arbeit/elastic-ai.runtime.enV5/python_utils/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Arbeit/elastic-ai.runtime.enV5/python_utils/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Arbeit/elastic-ai.runtime.enV5/python_utils/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:616\u001B[39m, in \u001B[36mMSELoss.forward\u001B[39m\u001B[34m(self, input, target)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Arbeit/elastic-ai.runtime.enV5/python_utils/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3868\u001B[39m, in \u001B[36mmse_loss\u001B[39m\u001B[34m(input, target, size_average, reduce, reduction, weight)\u001B[39m\n\u001B[32m   3865\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3866\u001B[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001B[32m-> \u001B[39m\u001B[32m3868\u001B[39m expanded_input, expanded_target = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3870\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m weight.size() != \u001B[38;5;28minput\u001B[39m.size():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Arbeit/elastic-ai.runtime.enV5/python_utils/.venv/lib/python3.12/site-packages/torch/functional.py:77\u001B[39m, in \u001B[36mbroadcast_tensors\u001B[39m\u001B[34m(*tensors)\u001B[39m\n\u001B[32m     75\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[32m     76\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T13:56:39.648482Z",
     "start_time": "2025-09-29T13:56:39.599271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "x = Tensor([[[0., 1., 2., 3., 4., 5.],      # channel 0\n",
    "             [0., -1., -2., -3., -4., -5.]]])  # channel 1\n",
    "\n",
    "x.requires_grad = True\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=3,\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    dilation=2,\n",
    "    padding=1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = Sequential(conv1d)\n",
    "model[0].weight = Parameter(\n",
    "    Tensor([[[-1., 0., 1.], [-1., 2., 3.]], \n",
    "            [[0., 0., 0.], [1., -1., -2.]], \n",
    "            [[-2., -1., -2.], [-3., -1., 0.]]]))\n",
    "model[0].bias = Parameter(Tensor([0.,1.,2.]))\n",
    "\n",
    "#print(\"\\nWeight shape:\", conv1d.weight.shape)\n",
    "#print(\"\\nBias:\", conv1d.bias.tolist())\n",
    "\n",
    "x = Tensor([[ [0.,1.,2.,3.,4.,5.], [0.,-1.,-2.,-3.,-4.,-5.] ]])\n",
    "x.requires_grad = True\n",
    "y = model(x)\n",
    "print(y)\n",
    "print(\"Output shape:\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "with torch.no_grad():\n",
    "    target = Tensor([[ [-4.,-3.],[-2.,-1.], [0.,1.] ]])\n",
    "print(target)\n",
    "loss = loss_fn(y, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(\"FIRST CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n",
    "print(\"\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(\"SECOND CONVOLUTION\")\n",
    "print(\"Gradient wrt x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient wrt weights:\")\n",
    "print(model[0].weight.grad)\n",
    "print(\"Gradient wrt bias:\")\n",
    "print(model[0].bias.grad)\n"
   ],
   "id": "2ef9fed02c5ca971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 2, 6])\n",
      "tensor([[[-8., -8.],\n",
      "         [ 8.,  3.],\n",
      "         [-4.,  4.]]], grad_fn=<ConvolutionBackward0>)\n",
      "Output shape: torch.Size([1, 3, 2])\n",
      "tensor([[[-4., -3.],\n",
      "         [-2., -1.],\n",
      "         [ 0.,  1.]]])\n",
      "tensor(30.3333, grad_fn=<MseLossBackward0>)\n",
      "FIRST CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[  0.0000,   1.3333,  -0.3333,   1.3333,  -1.0000,   0.0000],\n",
      "         [  0.0000,  -4.6667,   0.0000, -10.6667,  -5.6667,   0.0000]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ -3.3333,  -8.0000,  -4.0000],\n",
      "         [  3.3333,   8.0000,   4.0000]],\n",
      "\n",
      "        [[  2.6667,   8.6667,  10.0000],\n",
      "         [ -2.6667,  -8.6667, -10.0000]],\n",
      "\n",
      "        [[  2.0000,   2.6667,  -4.0000],\n",
      "         [ -2.0000,  -2.6667,   4.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([-3.0000,  4.6667, -0.3333])\n",
      "\n",
      "--------------------------\n",
      "SECOND CONVOLUTION\n",
      "Gradient wrt x:\n",
      "tensor([[[  0.0000,   1.3333,  -0.3333,   1.3333,  -1.0000,   0.0000],\n",
      "         [  0.0000,  -4.6667,   0.0000, -10.6667,  -5.6667,   0.0000]]])\n",
      "Gradient wrt weights:\n",
      "tensor([[[ -3.3333,  -8.0000,  -4.0000],\n",
      "         [  3.3333,   8.0000,   4.0000]],\n",
      "\n",
      "        [[  2.6667,   8.6667,  10.0000],\n",
      "         [ -2.6667,  -8.6667, -10.0000]],\n",
      "\n",
      "        [[  2.0000,   2.6667,  -4.0000],\n",
      "         [ -2.0000,  -2.6667,   4.0000]]])\n",
      "Gradient wrt bias:\n",
      "tensor([-3.0000,  4.6667, -0.3333])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:10:06.973246Z",
     "start_time": "2025-09-29T14:10:06.967593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = torch.tensor([1., 2., -3.], requires_grad=True)\n",
    "label = torch.tensor([-5., -4., 2.])\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "loss = loss_fn(output, label)\n",
    "loss.backward()\n",
    "\n",
    "print(output.grad)\n"
   ],
   "id": "3519318d9a898a03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.0000,  4.0000, -3.3333])\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcb54b01cbe0a16d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
